{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGCVVQDmSQx9"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = np.array(x_train,dtype = np.float64)\n",
        "x_test = np.array(x_test,dtype = np.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vTmLPDnSe-k"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vb91MSHSyRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "9082694a-7288-4c32-ccc2-1edec22022a8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "index = 27\n",
        "print(y_train[index])\n",
        "print(plt.imshow(x_train[index]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "AxesImage(54,36;334.8x217.44)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDxdnDx-ThKP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cfe8b6ec-5274-44a5-ea11-b5e9ad7f8f3a"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB3TIM9BTqmx"
      },
      "source": [
        "# save input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0],img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],img_rows, img_cols, 1)\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTNEw4zQUi79"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train,num_classes)\n",
        "y_test = to_categorical(y_test,num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-0tAbt5WOkQ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, MaxPooling2D , Conv2D\n",
        "model = Sequential() # empty NN model - basis for adding other layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbPIzAviYaHr"
      },
      "source": [
        "# adding the first convolutional layer\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "     activation='relu',\n",
        "     input_shape=(img_rows, img_cols, 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPcXS-qYXpP3"
      },
      "source": [
        "# adding second convolutional layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIEFEpMwX5aU"
      },
      "source": [
        "# adding a pooling layer\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzUAt_YOYOFf"
      },
      "source": [
        "# droping out some units in order to avoid overfitting\n",
        "model.add(Dropout(0.25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgO9bCkoYXF0"
      },
      "source": [
        "# adding a flattening layer to convert the previous hidden layer in to a 1D array\n",
        "model.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AKjUQjtY_Dp"
      },
      "source": [
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiAlS4xcZrOK"
      },
      "source": [
        "# The “softmax” activation is used when we’d like to classify the data into a number of pre-decided classes.\n",
        "model.add(Dense(num_classes, activation = 'softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC7U3WJDst8k"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfw7s1vFsvWD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "36ef5a87-a758-4aae-ab8d-c2a78e69b423"
      },
      "source": [
        "batch_size = 128 # The batch size is a number of samples processed before the model is updated.\n",
        "epochs = 10 # The number of epochs is the number of complete passes through the training dataset.\n",
        "model.fit(x_train,y_train,batch_size=batch_size, epochs = epochs, verbose = 1,validation_data = (x_test,y_test))\n",
        "score = model.evaluate(x_test,y_test, verbose=0)\n",
        "print(\"test loss : \",score[0])\n",
        "print(\"test accuracy : \",score[1])\n",
        "model.save(\"test_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 138s 294ms/step - loss: 0.3436 - accuracy: 0.8048 - val_loss: 0.3311 - val_accuracy: 0.8054\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 136s 290ms/step - loss: 0.3326 - accuracy: 0.8050 - val_loss: 0.3261 - val_accuracy: 0.8064\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 135s 289ms/step - loss: 0.3272 - accuracy: 0.8076 - val_loss: 0.3219 - val_accuracy: 0.8076\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 138s 293ms/step - loss: 0.3203 - accuracy: 0.8121 - val_loss: 0.3102 - val_accuracy: 0.8107\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 138s 293ms/step - loss: 0.3038 - accuracy: 0.8528 - val_loss: 0.2665 - val_accuracy: 0.9035\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 138s 294ms/step - loss: 0.2469 - accuracy: 0.9043 - val_loss: 0.2263 - val_accuracy: 0.9045\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 138s 294ms/step - loss: 0.2217 - accuracy: 0.9089 - val_loss: 0.2133 - val_accuracy: 0.9031\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 138s 294ms/step - loss: 0.1986 - accuracy: 0.9362 - val_loss: 0.1741 - val_accuracy: 0.9826\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 137s 293ms/step - loss: 0.1533 - accuracy: 0.9855 - val_loss: 0.1350 - val_accuracy: 0.9903\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 138s 294ms/step - loss: 0.1231 - accuracy: 0.9896 - val_loss: 0.1125 - val_accuracy: 0.9911\n",
            "test loss :  0.1124848872423172\n",
            "test accuracy :  0.991100013256073\n",
            "INFO:tensorflow:Assets written to: test_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}